{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "# TODO: model must be evaluated eval() after loading\n",
    "\n",
    "DATE = '2020-07-22'#'2020-07-17'\n",
    "EXP = '2_n_ep1000_l_ep313_lr0.001'\n",
    "LOAD_DIR = Path('../saved_data') / DATE / EXP\n",
    "\n",
    "# load general data\n",
    "config = torch.load(LOAD_DIR / 'config')\n",
    "total_rewards = torch.load(LOAD_DIR / 'tot_r')\n",
    "times = torch.load(LOAD_DIR / 'running_times')\n",
    "\n",
    "# load state data\n",
    "versions = ['optim1']#, 'optim100']\n",
    "states = dict()\n",
    "plots = []\n",
    "fig_count = 0\n",
    "for version in versions:\n",
    "    states.update({version: torch.load(LOAD_DIR / ('batch_states_' + version))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.00000000e+00, 9.50000000e-01, 5.00000000e-02, 0.00000000e+00],\n",
       "        [1.00000000e+00, 9.50000000e-01, 4.50000000e-02, 5.00000000e-03],\n",
       "        [1.00000000e+00, 9.55000000e-01, 4.50000000e-02, 0.00000000e+00],\n",
       "        ...,\n",
       "        [1.00000000e+00, 9.85000000e-01, 1.00000000e-02, 5.00000000e-03],\n",
       "        [1.00000000e+00, 9.85000000e-01, 5.00000000e-03, 1.00000000e-02],\n",
       "        [1.00000000e+00, 9.90000000e-01, 5.00000000e-03, 5.00000000e-03]],\n",
       "\n",
       "       [[1.00000000e+00, 9.50000000e-01, 5.00000000e-02, 0.00000000e+00],\n",
       "        [1.00000000e+00, 9.50000000e-01, 5.00000000e-02, 0.00000000e+00],\n",
       "        [1.00000000e+00, 9.45000000e-01, 5.00000000e-02, 5.00000000e-03],\n",
       "        ...,\n",
       "        [1.00000000e+00, 9.75000000e-01, 1.50000000e-02, 1.00000000e-02],\n",
       "        [1.00000000e+00, 9.70000000e-01, 1.50000000e-02, 1.50000000e-02],\n",
       "        [1.00000000e+00, 9.65000000e-01, 1.50000000e-02, 2.00000000e-02]],\n",
       "\n",
       "       [[1.00000000e+00, 9.50000000e-01, 5.00000000e-02, 0.00000000e+00],\n",
       "        [1.00000000e+00, 9.55000000e-01, 4.50000000e-02, 0.00000000e+00],\n",
       "        [1.00000000e+00, 9.50000000e-01, 5.00000000e-02, 0.00000000e+00],\n",
       "        ...,\n",
       "        [1.00000000e+00, 9.40000000e-01, 3.00000000e-02, 3.00000000e-02],\n",
       "        [1.00000000e+00, 9.40000000e-01, 3.00000000e-02, 3.00000000e-02],\n",
       "        [1.00000000e+00, 9.40000000e-01, 3.00000000e-02, 3.00000000e-02]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1.00000000e+00, 9.50000000e-01, 5.00000000e-02, 0.00000000e+00],\n",
       "        [1.00000000e+00, 9.45000000e-01, 5.50000000e-02, 0.00000000e+00],\n",
       "        [1.00000000e+00, 9.40000000e-01, 6.00000000e-02, 0.00000000e+00],\n",
       "        ...,\n",
       "        [1.00000000e+00, 9.75000000e-01, 1.50000000e-02, 1.00000000e-02],\n",
       "        [1.00000000e+00, 9.80000000e-01, 1.00000000e-02, 1.00000000e-02],\n",
       "        [1.00000000e+00, 9.85000000e-01, 1.00000000e-02, 5.00000000e-03]],\n",
       "\n",
       "       [[1.00000000e+00, 9.50000000e-01, 5.00000000e-02, 0.00000000e+00],\n",
       "        [1.00000000e+00, 9.55000000e-01, 4.50000000e-02, 0.00000000e+00],\n",
       "        [1.00000000e+00, 9.50000000e-01, 5.00000000e-02, 0.00000000e+00],\n",
       "        ...,\n",
       "        [1.00000000e+00, 9.90000000e-01, 5.00000000e-03, 5.00000000e-03],\n",
       "        [1.00000000e+00, 9.90000000e-01, 5.00000000e-03, 5.00000000e-03],\n",
       "        [1.00000000e+00, 9.90000000e-01, 5.00000000e-03, 5.00000000e-03]],\n",
       "\n",
       "       [[1.00000000e+00, 9.50000000e-01, 5.00000000e-02, 0.00000000e+00],\n",
       "        [1.00000000e+00, 9.55000000e-01, 4.50000000e-02, 0.00000000e+00],\n",
       "        [1.00000000e+00, 9.50000000e-01, 5.00000000e-02, 0.00000000e+00],\n",
       "        ...,\n",
       "        [1.00000000e+00, 9.30000000e-01, 5.00000000e-03, 6.50000000e-02],\n",
       "        [1.00000000e+00, 9.35000000e-01, 1.21430643e-17, 6.50000000e-02],\n",
       "        [1.00000000e+00, 9.30000000e-01, 1.21430643e-17, 7.00000000e-02]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states[version]['FSC']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>plots for total reward </center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot total rewards of active agents and the rolling mean\n",
    "window_width = 50\n",
    "\n",
    "fig = plt.figure(fig_count, figsize=(12, 4))\n",
    "plots.append(fig)\n",
    "ax = [fig.add_subplot(121), fig.add_subplot(122)]\n",
    "fig_count += 1\n",
    "\n",
    "for i, agt in enumerate(config['active_agents']):\n",
    "    ma = pd.Series(total_rewards[agt]).rolling(window_width).mean()\n",
    "    ax[i].plot(total_rewards[agt])\n",
    "    ax[i].plot(ma, label='{} ep - ma'.format(window_width))\n",
    "    ax[i].set_title(agt)\n",
    "    ax[i].set_xlabel('episode')\n",
    "    ax[i].set_ylabel('reward')\n",
    "    ax[i].legend()\n",
    "fig.suptitle('reward of agent')\n",
    "fig.text(0.93, 0.4, r'$\\beta_{FSC}$:' + '\\t {}'.format(config['support_factor']) + '\\n' +\n",
    "                    r'$\\beta_{j}$:' + ' \\t  {}'.format(config['support_factor'] * config['ratio of support factor']) + '\\n' +\n",
    "                    r'$\\Delta_{\\rho}$:' + '     {}'.format(config['delta_resource'])  + '\\n' +\n",
    "                     'sub lvl:' + '   {}'.format(config['sub_lvl'])  + '\\n\\n' +\n",
    "                     'batch size:' + '       {}'.format(config['batch_size']) + '\\n' +\n",
    "                     'learning rate:' + '    {}'.format(config['lr']) + '\\n' +\n",
    "                     'discout factor:' + '   {}'.format(config['gamma']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>network analysis</center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0,  2,  7, 15], dtype=int64),)\n",
      "[[ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00  1.4901161e-08 -2.9802322e-08]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00  1.4901161e-08 -2.9802322e-08]\n",
      " [ 0.0000000e+00  0.0000000e+00 -1.4901161e-08  8.9406967e-08]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 -1.9988418e-03]\n",
      " [-1.4901161e-08  0.0000000e+00 -8.9406967e-08  1.4156103e-07]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00 -2.9802322e-08  2.9802322e-08]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 -5.9604645e-08]\n",
      " [ 0.0000000e+00  0.0000000e+00  3.7252903e-09 -2.9802322e-08]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.9802322e-08]\n",
      " [ 0.0000000e+00  0.0000000e+00  5.9604645e-08  8.5681677e-08]\n",
      " [ 7.4505806e-09  0.0000000e+00  8.9406967e-08 -3.1292439e-07]\n",
      " [ 1.9999743e-03  1.9999743e-03  1.9994229e-03 -8.9406967e-08]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00]]\n",
      "(array([], dtype=int64),)\n",
      "[[ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   1.4901161e-08  0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  -1.4901161e-08  0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  -1.9998811e-03  0.0000000e+00  0.0000000e+00  0.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# determine dead neurons by calculation of difference of weights\n",
    "agents = [torch.load(LOAD_DIR / 'agents_init'), torch.load(LOAD_DIR / 'agents_optim1_ep10')]\n",
    "weights_diff = []\n",
    "weights_diff.append(agents[0]['FSC'].get_networks()['Shell'][0].weight.detach().numpy() - \\\n",
    "                   agents[1]['FSC'].get_networks()['Shell'][0].weight.detach().numpy())\n",
    "weights_diff.append(agents[0]['FSC'].get_networks()['Shell'][2].weight.detach().numpy() - \\\n",
    "                   agents[1]['FSC'].get_networks()['Shell'][2].weight.detach().numpy())\n",
    "for i in weights_diff:\n",
    "    print(np.where(np.all(np.isclose(i, 0), axis=1)))\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>plots for support</center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over all versions\n",
    "for version in versions:\n",
    "    fig = plt.figure(fig_count, figsize=(12, 4))\n",
    "    plots.append(fig)\n",
    "    fig_count += 1\n",
    "    # create two axes\n",
    "    ax = [fig.add_subplot(121), fig.add_subplot(122)]\n",
    "    # loop over each agent\n",
    "    for key in ['Shell', 'Gov']:#config['agents']:#\n",
    "        # calculate mean and std for support\n",
    "        mean = states[version][key].mean(axis=0)[:, 0]\n",
    "        std = states[version][key].std(axis=0)[:, 0]\n",
    "        x = range(0, states[version][key].shape[1])\n",
    "        # plot mean\n",
    "        ax[0].plot(mean, label=key)\n",
    "        ax[1].plot(mean, label=key)\n",
    "        # plot std as an area\n",
    "        ax[0].fill_between(x, mean+std, mean-std, alpha=0.2)\n",
    "        # plot max and min as an area\n",
    "        ax[1].fill_between(x, states[version][key].max(axis=0)[:, 0],\n",
    "                           states[version][key].min(axis=0)[:, 0], alpha=0.2)\n",
    "    # set some plot properties\n",
    "    fig.suptitle('support of agents of batch - ' + version)\n",
    "    fig.text(0.93, 0.4, r'$\\beta_{FSC}$:' + '\\t {}'.format(config['support_factor']) + '\\n' +\n",
    "                        r'$\\beta_{j}$:' + ' \\t  {}'.format(config['support_factor'] * config['ratio of support factor']) + '\\n' +\n",
    "                        r'$\\Delta_{\\rho}$:' + '     {}'.format(config['delta_resource'])  + '\\n' +\n",
    "                         'sub lvl:' + '   {}'.format(config['sub_lvl'])  + '\\n\\n' +\n",
    "                         'batch size:' + '       {}'.format(config['batch_size']) + '\\n' +\n",
    "                         'learning rate:' + '    {}'.format(config['lr']) + '\\n' +\n",
    "                         'discout factor:' + '   {}'.format(config['gamma']))\n",
    "    ax[0].set_title('mean and standard deviation')\n",
    "    ax[1].set_title('mean and max-min')\n",
    "    for axis in fig.get_axes():\n",
    "#         axis.set_ylim(ymax=0.3)\n",
    "        axis.set_xlabel('step')\n",
    "        axis.set_ylabel('level of support')\n",
    "        axis.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>plots for resource assignment</center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for version in versions:\n",
    "    fig = plt.figure(fig_count, figsize=(12, 4))\n",
    "    plots.append(fig)\n",
    "    # create new axes\n",
    "    ax = [fig.add_subplot(121), fig.add_subplot(122)]\n",
    "    fig_count += 1\n",
    "\n",
    "    for i, key in enumerate(config['active_agents']):\n",
    "        # calculate mean of resource assignment to the different partner agents\n",
    "        x = range(0, states[version][key].shape[1])\n",
    "        fsc = states[version][key].mean(axis=0)[:, 1]\n",
    "        shell = states[version][key].mean(axis=0)[:, 2]\n",
    "        gov = states[version][key].mean(axis=0)[:, 3]\n",
    "\n",
    "        # create stacked plots\n",
    "        ax[i].stackplot(x, gov + shell + fsc)\n",
    "        ax[i].stackplot(x, gov + shell)\n",
    "        ax[i].stackplot(x, gov)\n",
    "        ax[i].set_title(key)\n",
    "        ax[i].set_xlabel('step')\n",
    "        ax[i].legend(['FSC', 'Shell', 'Gov'], loc='center')\n",
    "    fig.suptitle('mean resource allocation for batch - ' + version)\n",
    "    fig.text(0.93, 0.4, r'$\\beta_{FSC}$:' + '\\t {}'.format(config['support_factor']) + '\\n' +\n",
    "                    r'$\\beta_{j}$:' + ' \\t  {}'.format(config['support_factor'] * config['ratio of support factor']) + '\\n' +\n",
    "                    r'$\\Delta_{\\rho}$:' + '     {}'.format(config['delta_resource'])  + '\\n' +\n",
    "                     'sub lvl:' + '   {}'.format(config['sub_lvl'])  + '\\n\\n' +\n",
    "                     'batch size:' + '       {}'.format(config['batch_size']) + '\\n' +\n",
    "                     'learning rate:' + '    {}'.format(config['lr']) + '\\n' +\n",
    "                     'discout factor:' + '   {}'.format(config['gamma']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>plots for support calculation</center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "# %matplotlib inline\n",
    "support_calc = dict()\n",
    "par_agt = {'Shell': ['FSC', 'Gov'], 'Gov': ['FSC', 'Shell']}\n",
    "for version in versions:\n",
    "    # load data\n",
    "    support_calc.update({version: torch.load(LOAD_DIR / ('support_calc_' + version))})\n",
    "    # create figure and axis\n",
    "    fig = plt.figure(fig_count, figsize=(12, 4))    \n",
    "    plots.append(fig)  \n",
    "    fig_count += 1\n",
    "    ax = [fig.add_subplot(121), fig.add_subplot(122)]\n",
    "    \n",
    "    for i, key in enumerate(support_calc[version].keys()):\n",
    "        x = range(0, support_calc[version][key].shape[1])\n",
    "        mean = support_calc[version][key].mean(axis=0)[:, :]               \n",
    "#         ax[i].plot(mean[:,0], label='support before' ) # oder raus\n",
    "        ax[i].plot(mean[:,1], label= par_agt[key][0] + ' (partner)')\n",
    "        ax[i].plot(mean[:,2], label= par_agt[key][1] + ' (partner)')\n",
    "        ax[i].legend()\n",
    "        ax[i].set_title(key)\n",
    "        ax[i].set_ylim(ymax=0.004)\n",
    "    fig.suptitle('(mean of) shares of support calculation - ' + version)\n",
    "    fig.text(0.93, 0.4, r'$\\beta_{FSC}$:' + '\\t {}'.format(config['support_factor']) + '\\n' +\n",
    "                        r'$\\beta_{j}$:' + ' \\t  {}'.format(config['support_factor'] * config['ratio of support factor']) + '\\n' +\n",
    "                        r'$\\Delta_{\\rho}$:' + '     {}'.format(config['delta_resource'])  + '\\n' +\n",
    "                         'sub lvl:' + '   {}'.format(config['sub_lvl'])  + '\\n\\n' +\n",
    "                         'batch size:' + '       {}'.format(config['batch_size']) + '\\n' +\n",
    "                         'learning rate:' + '    {}'.format(config['lr']) + '\\n' +\n",
    "                         'discout factor:' + '   {}'.format(config['gamma']))                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# support von Gov ist genau das gespiegelte von Shell\n",
    "support_calc[version]['Gov'].mean(axis=0)[:,:][:,2] + support_calc[version]['Shell'].mean(axis=0)[:,:][:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>plots for shell reward calculation</center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reward_shell = dict()\n",
    "for version in versions:\n",
    "    # load data\n",
    "    reward_shell.update({version: torch.load(LOAD_DIR / ('reward_shell_calc_' + version))})\n",
    "    fig = plt.figure(fig_count)\n",
    "    fig_count += 1\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    mean = reward_shell[version].mean(axis=0)[:, :]\n",
    "    x = range(0, reward_shell[version].shape[1])\n",
    "    \n",
    "    # create stacked plots\n",
    "    ax.stackplot(x, mean[:, 3] + mean[:, 2] + mean[:, 1])\n",
    "    ax.stackplot(x, mean[:, 3] + mean[:, 2])\n",
    "    ax.stackplot(x, mean[:, 3])\n",
    "    \n",
    "    # axis properties\n",
    "    ax.legend(['own return', 'external spending', 'internal spending'])\n",
    "    ax.set_title('mean of shares of discounted reward for Shell - ' + version)\n",
    "    fig.text(0.93, 0.4, r'$\\beta_{FSC}$:' + '\\t {}'.format(config['support_factor']) + '\\n' +\n",
    "                    r'$\\beta_{j}$:' + ' \\t  {}'.format(config['support_factor'] * config['ratio of support factor']) + '\\n' +\n",
    "                    r'$\\Delta_{\\rho}$:' + '     {}'.format(config['delta_resource'])  + '\\n' +\n",
    "                     'sub lvl:' + '   {}'.format(config['sub_lvl'])  + '\\n\\n' +\n",
    "                     'batch size:' + '       {}'.format(config['batch_size']) + '\\n' +\n",
    "                     'learning rate:' + '    {}'.format(config['lr']) + '\\n' +\n",
    "                     'discout factor:' + '   {}'.format(config['gamma']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>saving plots to pdf or svg</center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, version in enumerate(versions):\n",
    "    plots[1+i].savefig((LOAD_DIR / (DATE + '_' + EXP + '_'  + versions[i] + '_support' + '.pdf')), bbox_inches=\"tight\") # svg auch m√∂glich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>load classes to load agents with pickle/torch</center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "# TODO: put get_actions and predict in Agent as they are the same for ?all? agents\n",
    "class Agent(object):\n",
    "    def __init__(self, act_space, n_state, device):\n",
    "        self.__state = None\n",
    "        self.__n_neurons = 16\n",
    "        self.__action_space = act_space\n",
    "        self._device = device\n",
    "\n",
    "        # Define network\n",
    "        self.__base_network = nn.Sequential(nn.Linear(n_state, self.__n_neurons),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Linear(self.__n_neurons, len(act_space)),\n",
    "                                            nn.Softmax(dim=-1))\n",
    "\n",
    "    def get_actions(self, state) -> dict:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_base_net(self):\n",
    "        return self.__base_network\n",
    "\n",
    "    def get_action_space(self):\n",
    "        return self.__action_space\n",
    "\n",
    "\n",
    "class Shell(Agent):\n",
    "    def __init__(self, action_space, n_state, act_partners, device):\n",
    "        super().__init__(action_space, n_state, device)\n",
    "        self.__networks = dict()\n",
    "\n",
    "        # initialize all necessary networks by copying the base network and send it to device\n",
    "        for agt in act_partners:\n",
    "            self.__networks.update({agt: copy.copy(super().get_base_net()).to(device)})\n",
    "\n",
    "    def get_actions(self, state) -> dict:\n",
    "        nets = self.__networks\n",
    "        actions = dict()\n",
    "\n",
    "        # derive an action for each network (i.e., policy)\n",
    "        for key in nets.keys():\n",
    "            # detach() should not be a problem hear, as for optimization predict() is called again,\n",
    "            # where no detach() is used\n",
    "            action_probs = self.predict(state, key).cpu().detach().numpy()\n",
    "            actions.update({key: np.random.choice(super().get_action_space(), p=action_probs)})\n",
    "        return actions\n",
    "\n",
    "    def predict(self, state, partner_agt):\n",
    "        # get the action probabilities as a tensor\n",
    "        action_probs = self.__networks[partner_agt](torch.FloatTensor(state).to(self._device))\n",
    "        return action_probs\n",
    "\n",
    "    def get_networks(self):\n",
    "        return self.__networks\n",
    "\n",
    "\n",
    "class FSC(Agent):\n",
    "    def __init__(self, action_space, n_state, act_partners, device):\n",
    "        super().__init__(action_space, n_state, device)\n",
    "        self.__networks = dict()\n",
    "        self.__action_space = action_space\n",
    "\n",
    "        # initialize all necessary networks by copying the base network and send it to device\n",
    "        for agt in act_partners:\n",
    "            self.__networks.update({agt: copy.copy(super().get_base_net()).to(device)})\n",
    "\n",
    "    def get_actions(self, state) -> dict:\n",
    "        nets = self.__networks\n",
    "        actions = dict()\n",
    "\n",
    "        # derive an action for each network (i.e., policy)\n",
    "        for key in nets.keys():\n",
    "            action_probs = self.predict(state, key).cpu().detach().numpy()\n",
    "            actions.update({key: np.random.choice(super().get_action_space(), p=action_probs)})\n",
    "        return actions\n",
    "\n",
    "    def predict(self, state, partner_agt):\n",
    "        # get the action probabilities as a tensor\n",
    "        action_probs = self.__networks[partner_agt](torch.FloatTensor(state).to(self._device))\n",
    "        return action_probs\n",
    "\n",
    "    def get_networks(self):\n",
    "        return self.__networks\n",
    "\n",
    "\n",
    "class Gov(Agent):\n",
    "    def __init__(self, action_space, n_state, activ_con, device):\n",
    "        # super().__init__(action_space, n_state)\n",
    "        pass\n",
    "\n",
    "    def get_actions(self, state) -> dict:\n",
    "        # government is performing maintain as action. That equals an passiv agent.\n",
    "        return {'Shell': 0, 'FSC': 0}\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
